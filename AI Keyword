statistical learning theory
structural risk minimization principle (SRM)
unshared convolution
Tiled convolution
knowledge base
machine learning: "AI systems need the ability to acquire their own knowledge, by extracting patterns from raw data. This capability is known as machine learning"
logistic regression
naive Bayes
representation of the data
representation learning
autoencoder
factors of variation
sampling
Wald test
likelihood ratio test
statistical test
local binary patterns
local ternary patterns
Allograph-base
receptive field
Spatial arrangement
depth
stride
zero-padding
Parameter Sharing
Dilated Convolution
no-op
Data Augmentation
Transfer Learning
object detection
object segmentation
image classification
object localization
Network in Network Layers
Dropout Layers
Pooling Layers
ReLU layers
vanishing gradient problem
Max-Norm
early stopping
Amdahlâ€™s Law
Generalization error
maxout unit
Tikhonov regularization := L2 weight regularization
lasso regularization
KL-sparsity = KL-divergence = Kullback-Leibler divergence
Monte-Carlo Model Averaging
Weight Scaling
ridge regression
lasso regression
permutation invariant := "In this context this refers to the fact that the model does not assume any spatial relationships between the features. E.g. for multilayer perceptron, you can permute the pixels and the performance would be the same. This is not the case for convolutional networks, which assume neighbourhood relations." ==  a model that produces the same output regardless of the order of elements in the input vector
